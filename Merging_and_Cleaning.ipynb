{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Download Duration: 1.031313419342041 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download raw data from postgres for stage 1 ETL\n",
    "\n",
    "conn_string = 'postgres://whnpmxwsiccrtg:53c453893549d2b1e6a4ff92e626a2a08ebcaff66678e50d33e3742f66e3e4f4@ec2-52-4-171-132.compute-1.amazonaws.com/d2ajro4cjr10lb'\n",
    "\n",
    "db = create_engine(conn_string)\n",
    "conn = db.connect()\n",
    "\n",
    "start_time = time.time()\n",
    "clean_listing = pd.read_sql_query('select * from \"clean_listing_remove_somereviews\"',con=conn)\n",
    "amenities = pd.read_sql_query('select * from \"amenities_bucketed\"',con=conn)\n",
    "print(\"PostGres Download Duration: {} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated variables.\n",
    "listing = clean_listing.drop(columns = ['last_scraped', 'host_since', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge amenities table to full listings.\n",
    "merged = listing.merge(amenities, how='left', on ='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables and drop 'id' (unique identifier - not relevant)\n",
    "merged = merged.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = merged.dtypes[merged.dtypes == 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, drop='if_binary')\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(merged[objects]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "merged = merged.merge(encode_df,left_index=True, right_index=True)\n",
    "merged = merged.drop(columns=objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set erroneous 30 bedroom listings for apartments to 1\n",
    "merged.loc[merged['bedrooms'] > 29, 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 2 bedrooms\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] > 4), 'bedrooms'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 1 bedroom\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] < 5), 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5899, 266)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from scipy import stats\n",
    "merged['accommodates_logs'] = np.log(merged['accommodates'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['accommodates_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 267)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "merged.loc[merged.bathrooms == 0, 'bathrooms'] = .001\n",
    "merged['baths_logs'] = np.log(merged['bathrooms'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['baths_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(columns=['baths_logs', 'accommodates_logs'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 265)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    5831.000000\n",
       "mean       83.447608\n",
       "std        33.504199\n",
       "min         0.000000\n",
       "25%        93.000000\n",
       "50%        98.000000\n",
       "75%        99.000000\n",
       "max       100.000000\n",
       "Name: review_scores_rating, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "merged['review_scores_rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "host_listings_count_logs\n",
      "(5831, 266)\n",
      "accommodates_logs\n",
      "(5831, 267)\n",
      "bathrooms_logs\n",
      "(5831, 268)\n",
      "bedrooms_logs\n",
      "(5820, 269)\n",
      "price_logs\n",
      "(5799, 270)\n",
      "security_deposit_logs\n",
      "(5799, 271)\n",
      "cleaning_fee_logs\n",
      "(5799, 272)\n",
      "number_of_reviews_logs\n",
      "(5799, 273)\n",
      "number_of_reviews_ltm_logs\n",
      "(5799, 274)\n",
      "review_scores_rating_logs\n",
      "(5799, 275)\n",
      "review_scores_accuracy_logs\n",
      "(5799, 276)\n",
      "guests_included_logs\n",
      "(5799, 277)\n",
      "availability_30_logs\n",
      "(5799, 278)\n",
      "availability_60_logs\n",
      "(5799, 279)\n",
      "availability_90_logs\n",
      "(5799, 280)\n",
      "availability_365_logs\n",
      "(5799, 281)\n",
      "reviews_per_month_logs\n",
      "(5799, 282)\n",
      "days_host_logs\n",
      "(5706, 283)\n"
     ]
    }
   ],
   "source": [
    "# For loop to delete any rows with outliers in any row (3 SD) \n",
    "from scipy import stats\n",
    "log_column_list = []\n",
    "\n",
    "for column in merged.columns:\n",
    "    log_col_name = column + \"_logs\"\n",
    "    # Ignore columns with max less than or equal to 1 (binary)\n",
    "    if merged[column].max() > 1:\n",
    "        # natural log transform (+1 to handle 0 values)\n",
    "        merged[log_col_name] = np.log(merged[column]+1)\n",
    "        merged = merged[(np.abs(stats.zscore(merged[log_col_name])) < 3)]\n",
    "        log_column_list.append(log_col_name)\n",
    "        print(log_col_name)\n",
    "        print(merged.shape)\n",
    "\n",
    "merged.drop(columns=log_column_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional outliers using IsolationForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "X = merged.drop(columns=['price']).values\n",
    "iso = IsolationForest(contamination='auto')\n",
    "yhat = iso.fit_predict(X)\n",
    "merged['outlier'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any rows with outliers in any row (3 SD) using calculated field log(price/accommodates)\n",
    "merged = merged[(np.abs(stats.zscore(np.log(merged['price']/merged['accommodates']))) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged.drop(columns=['price']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged['outlier']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Upload Duration: 142.7544882297516 seconds\n"
     ]
    }
   ],
   "source": [
    "# Upload Merged dataset with errors corrected to PostGres\n",
    "\n",
    "start_time = time.time()\n",
    "merged.to_sql('merged_errors_corrected', con=conn, if_exists='replace', index=False)\n",
    "print(\"PostGres Upload Duration: {} seconds\".format(time.time() - start_time))\n",
    "conn.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}