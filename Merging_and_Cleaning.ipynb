{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Download Duration: 1.0560097694396973 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download raw data from postgres for stage 1 ETL\n",
    "\n",
    "conn_string = 'postgres://whnpmxwsiccrtg:53c453893549d2b1e6a4ff92e626a2a08ebcaff66678e50d33e3742f66e3e4f4@ec2-52-4-171-132.compute-1.amazonaws.com/d2ajro4cjr10lb'\n",
    "\n",
    "db = create_engine(conn_string)\n",
    "conn = db.connect()\n",
    "\n",
    "start_time = time.time()\n",
    "clean_listing = pd.read_sql_query('select * from \"clean_listing_remove_somereviews\"',con=conn)\n",
    "amenities = pd.read_sql_query('select * from \"amenities_bucketed\"',con=conn)\n",
    "print(\"PostGres Download Duration: {} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated variables.\n",
    "listing = clean_listing.drop(columns = ['last_scraped', 'host_since', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge amenities table to full listings.\n",
    "merged = listing.merge(amenities, how='left', on ='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables and drop 'id' (unique identifier - not relevant)\n",
    "merged = merged.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = merged.dtypes[merged.dtypes == 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, drop='if_binary')\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(merged[objects]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "merged = merged.merge(encode_df,left_index=True, right_index=True)\n",
    "merged = merged.drop(columns=objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set erroneous 30 bedroom listings for apartments to 1\n",
    "merged.loc[merged['bedrooms'] > 29, 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 2 bedrooms\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] > 4), 'bedrooms'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 1 bedroom\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] < 5), 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5899, 264)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from scipy import stats\n",
    "merged['accommodates_logs'] = np.log(merged['accommodates'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['accommodates_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 265)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "merged.loc[merged.bathrooms == 0, 'bathrooms'] = .001\n",
    "merged['baths_logs'] = np.log(merged['bathrooms'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['baths_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(columns=['baths_logs', 'accommodates_logs'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 263)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    0.0\n",
       "mean     NaN\n",
       "std      NaN\n",
       "min      NaN\n",
       "25%      NaN\n",
       "50%      NaN\n",
       "75%      NaN\n",
       "max      NaN\n",
       "Name: review_scores_rating, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "merged['review_scores_rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "host_listings_count_logs\n(5831, 264)\naccommodates_logs\n(5831, 265)\nbathrooms_logs\n(5831, 266)\nbedrooms_logs\n(5820, 267)\nprice_logs\n(5799, 268)\nsecurity_deposit_logs\n(5799, 269)\ncleaning_fee_logs\n(5799, 270)\nreview_scores_rating_logs\n(0, 271)\n"
     ]
    }
   ],
   "source": [
    "# For loop to delete any rows with outliers in any row (3 SD) \n",
    "from scipy import stats\n",
    "log_column_list = []\n",
    "\n",
    "for column in merged.columns:\n",
    "    log_col_name = column + \"_logs\"\n",
    "    # Ignore columns with max less than or equal to 1 (binary)\n",
    "    if merged[column].max() > 1:\n",
    "        # natural log transform (+1 to handle 0 values)\n",
    "        merged[log_col_name] = np.log(merged[column]+1)\n",
    "        merged = merged[(np.abs(stats.zscore(merged[log_col_name])) < 3)]\n",
    "        log_column_list.append(log_col_name)\n",
    "        print(log_col_name)\n",
    "        print(merged.shape)\n",
    "\n",
    "merged.drop(columns=log_column_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 262)) while a minimum of 1 is required.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-636f82e3084f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0miso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miso\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'outlier'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \"\"\"\n\u001b[0;32m    739\u001b[0m         \u001b[1;31m# override for transductive outlier detectors like LocalOulierFactor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    261\u001b[0m                 )\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    651\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[1;32m--> 653\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 262)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# Drop additional outliers using IsolationForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "X = merged.drop(columns=['price']).values\n",
    "iso = IsolationForest(contamination='auto')\n",
    "yhat = iso.fit_predict(X)\n",
    "merged['outlier'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\golfz\\anaconda3\\envs\\PythonData\\lib\\site-packages\\scipy\\stats\\stats.py:2497: RuntimeWarning: Mean of empty slice.\n  mns = a.mean(axis=axis, keepdims=True)\nC:\\Users\\golfz\\anaconda3\\envs\\PythonData\\lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n  ret, rcount, out=ret, casting='unsafe', subok=False)\nC:\\Users\\golfz\\anaconda3\\envs\\PythonData\\lib\\site-packages\\numpy\\core\\_methods.py:234: RuntimeWarning: Degrees of freedom <= 0 for slice\n  keepdims=keepdims)\nC:\\Users\\golfz\\anaconda3\\envs\\PythonData\\lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in true_divide\n  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\nC:\\Users\\golfz\\anaconda3\\envs\\PythonData\\lib\\site-packages\\numpy\\core\\_methods.py:224: RuntimeWarning: invalid value encountered in true_divide\n  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "# Delete any rows with outliers in any row (3 SD) using calculated field log(price/accommodates)\n",
    "merged = merged[(np.abs(stats.zscore(np.log(merged['price']/merged['accommodates']))) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged.drop(columns=['price']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged['outlier']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Upload Duration: 140.9793496131897 seconds\n"
     ]
    }
   ],
   "source": [
    "# Upload Merged dataset with errors corrected to PostGres\n",
    "\n",
    "start_time = time.time()\n",
    "merged.to_sql('merged_errors_corrected', con=conn, if_exists='replace', index=False)\n",
    "print(\"PostGres Upload Duration: {} seconds\".format(time.time() - start_time))\n",
    "conn.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}