{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Download Duration: 1.1348206996917725 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download raw data from postgres for stage 1 ETL\n",
    "\n",
    "conn_string = 'postgres://whnpmxwsiccrtg:53c453893549d2b1e6a4ff92e626a2a08ebcaff66678e50d33e3742f66e3e4f4@ec2-52-4-171-132.compute-1.amazonaws.com/d2ajro4cjr10lb'\n",
    "\n",
    "db = create_engine(conn_string)\n",
    "conn = db.connect()\n",
    "\n",
    "start_time = time.time()\n",
    "clean_listing = pd.read_sql_query('select * from \"clean_listing_remove_somereviews\"',con=conn)\n",
    "amenities = pd.read_sql_query('select * from \"amenities_bucketed\"',con=conn)\n",
    "print(\"PostGres Download Duration: {} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated variables.\n",
    "listing = clean_listing.drop(columns = ['last_scraped', 'host_since', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge amenities table to full listings.\n",
    "merged = listing.merge(amenities, how='left', on ='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables and drop 'id' (unique identifier - not relevant)\n",
    "merged = merged.drop(columns = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = merged.dtypes[merged.dtypes == 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, drop='if_binary')\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(merged[objects]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "merged = merged.merge(encode_df,left_index=True, right_index=True)\n",
    "merged = merged.drop(columns=objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set erroneous 30 bedroom listings for apartments to 1\n",
    "merged.loc[merged['bedrooms'] > 29, 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 2 bedrooms\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] > 4), 'bedrooms'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zero bedrooms with more than 4 accommodates to 1 bedroom\n",
    "merged.loc[(merged['bedrooms'] == 0) & (merged['accommodates'] < 5), 'bedrooms'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5899, 258)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "from scipy import stats\n",
    "merged['accommodates_logs'] = np.log(merged['accommodates'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['accommodates_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 259)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "merged.loc[merged.bathrooms == 0, 'bathrooms'] = .001\n",
    "merged['baths_logs'] = np.log(merged['bathrooms'])\n",
    "merged = merged[(np.abs(stats.zscore(merged['baths_logs'])) < 2)]\n",
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(columns=['baths_logs', 'accommodates_logs'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5831, 257)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "host_listings_count_logs\n(5831, 258)\naccommodates_logs\n(5831, 259)\nbathrooms_logs\n(5831, 260)\nbedrooms_logs\n(5820, 261)\nprice_logs\n(5799, 262)\nsecurity_deposit_logs\n(5799, 263)\ncleaning_fee_logs\n(5799, 264)\nreview_scores_rating_logs\n(5799, 265)\nnumber_of_reviews_logs\n(5799, 266)\ndays_host_logs\n(5706, 267)\n"
     ]
    }
   ],
   "source": [
    "# For loop to delete any rows with outliers in any row (3 SD) \n",
    "from scipy import stats\n",
    "log_column_list = []\n",
    "\n",
    "for column in merged.columns:\n",
    "    log_col_name = column + \"_logs\"\n",
    "    # Ignore columns with max less than or equal to 1 (binary)\n",
    "    if merged[column].max() > 1:\n",
    "        # natural log transform (+1 to handle 0 values)\n",
    "        merged[log_col_name] = np.log(merged[column]+1)\n",
    "        merged = merged[(np.abs(stats.zscore(merged[log_col_name])) < 3)]\n",
    "        log_column_list.append(log_col_name)\n",
    "        print(log_col_name)\n",
    "        print(merged.shape)\n",
    "\n",
    "merged.drop(columns=log_column_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any rows with outliers in any row (3 SD) using calculated field log(price/accommodates)\n",
    "merged = merged[(np.abs(stats.zscore(np.log(merged['price']/merged['accommodates']))) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop additional outliers using IsolationForest\n",
    "from sklearn.ensemble import IsolationForest\n",
    "X = merged.drop(columns=['price']).values\n",
    "iso = IsolationForest(contamination='auto')\n",
    "yhat = iso.fit_predict(X)\n",
    "merged['outlier'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[merged['outlier']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PostGres Upload Duration: 140.9793496131897 seconds\n"
     ]
    }
   ],
   "source": [
    "# Upload Merged dataset with errors corrected to PostGres\n",
    "\n",
    "start_time = time.time()\n",
    "merged.to_sql('merged_errors_corrected', con=conn, if_exists='replace', index=False)\n",
    "print(\"PostGres Upload Duration: {} seconds\".format(time.time() - start_time))\n",
    "conn.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}